{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import transformers\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import seaborn as sns\n",
    "\n",
    "#import torch\n",
    "#from huggingface_hub import notebook_login\n",
    "#from transformers import pipeline\n",
    "#from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stoplist=stopwords.words('english')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer= WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "import gensim\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing gensim related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1505387/3005824122.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df_orig = pd.read_csv(path, sep='|', error_bad_lines=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['statement', 'sentiment', 'topic', 'url'], dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = '../../datasets/test_dataset_model_pipeline/collected_statements_v0.5.csv'\n",
    "path = '../../datasets/test_dataset_model_pipeline/future_statements.csv'\n",
    "\n",
    "df_orig = pd.read_csv(path, sep='|', error_bad_lines=False)\n",
    "df_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is used to get the part-of-speech(POS) for lemmatization\n",
    "\"\"\"\n",
    "def get_tags(tag):\n",
    "   if tag.startswith('N') or tag.startswith('J'):\n",
    "      return wordnet.NOUN\n",
    "   #elif tag.startswith('J'):\n",
    "   #   return wordnet.ADJ\n",
    "   elif tag.startswith('V'):\n",
    "      return wordnet.VERB\n",
    "   elif tag.startswith('R'):\n",
    "      return wordnet.ADV\n",
    "   else:\n",
    "      return wordnet.NOUN #default case\n",
    "\n",
    "\"\"\"\n",
    "1. Removes Punctuations\n",
    "2. Removes words smaller than 3 letters\n",
    "3. Converts into lowercase\n",
    "4. Lemmatizes words\n",
    "5. Removes Stopwords\n",
    "\"\"\"\n",
    "def preprocess(text):\n",
    "   punctuation = list(string.punctuation)\n",
    "   doc_tokens = nltk.word_tokenize(text)\n",
    "   #print('doc_tokens', doc_tokens)\n",
    "   word_tokens = [word.lower() for word in doc_tokens if not (word in punctuation or len(word)<=3)]\n",
    "   # Lemmatize\n",
    "   _pos_tags = nltk.pos_tag(word_tokens)\n",
    "   pos_tags = []\n",
    "   for i in _pos_tags:\n",
    "      if re.search(r'(N)\\w+',i[1]):\n",
    "         pos_tags.append(i)\n",
    "   #print('pos_tags:',pos_tags)\n",
    "   doc_words = [wordnet_lemmatizer.lemmatize(word, pos = get_tags(tag)) for word, tag in pos_tags]\n",
    "   doc_words = [word for word in doc_words if word not in stoplist]\n",
    "   #print('doc_words', doc_words)\n",
    "   return doc_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_orig['statement'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig_s = df_orig.loc[5:8,:]\n",
    "# df_orig_s['cut'] = df_orig_s['statement'].apply(preprocess)\n",
    "# df_orig_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= list(df_clean)\n",
    "phrases = gensim.models.Phrases(docs, min_count=10, threshold=20)\n",
    "bigram_model = gensim.models.phrases.Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create bigrams from statements\n",
    "'''\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_model[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_mining -> 833\n",
      "test_data -> 704\n",
      "voice_recognition -> 470\n",
      "speech_recognition -> 426\n",
      "machine_translation -> 202\n",
      "face_recognition -> 195\n",
      "recognition_software -> 173\n",
      "computer_vision -> 140\n",
      "recognition_technology -> 126\n",
      "pattern_recognition -> 103\n",
      "language_processing -> 103\n",
      "machine_intelligence -> 90\n",
      "recognition_system -> 84\n",
      "training_data -> 72\n",
      "autopilot_system -> 69\n",
      "forex_autopilot -> 67\n",
      "computer_science -> 66\n",
      "data_data -> 62\n",
      "machine_learning -> 58\n",
      "supercomputer_center -> 50\n",
      "data_test -> 50\n",
      "money_autopilot -> 49\n",
      "search_engine -> 48\n",
      "mining_tool -> 47\n",
      "data_analysis -> 44\n",
      "business_intelligence -> 42\n",
      "game_player -> 40\n",
      "system_system -> 40\n",
      "sentiment_analysis -> 38\n",
      "intelligence_system -> 38\n",
      "business_autopilot -> 35\n",
      "player_game -> 35\n",
      "intelligence_intelligence -> 35\n",
      "information_technology -> 32\n",
      "technology_intelligence -> 31\n",
      "recognition_application -> 31\n",
      "mining_technique -> 30\n",
      "mining_data -> 29\n",
      "autopilot_cash -> 29\n",
      "system_data -> 28\n",
      "information_system -> 28\n",
      "game_game -> 27\n",
      "data_set -> 26\n",
      "player_player -> 26\n",
      "intelligence_technology -> 25\n",
      "recognition_program -> 25\n",
      "intelligence_technique -> 25\n",
      "review_forex -> 25\n",
      "machine_technique -> 24\n",
      "conference_intelligence -> 24\n"
     ]
    }
   ],
   "source": [
    "# Checkout most frequent bigrams\n",
    "bigram_counter1 = Counter()\n",
    "for key in phrases.vocab.keys():\n",
    "    if key not in stopwords.words('english'):\n",
    "        if len(str(key).split('_'))>1:\n",
    "            bigram_counter1[key]+=phrases.vocab[key]\n",
    "\n",
    "for key, counts in bigram_counter1.most_common(50):\n",
    "    print(key,\"->\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelstep: Feeding the bigrams into a Word2Vec model produces more meaningful bigrams\n",
    "w2vmodel = Word2Vec(sentences=bigram_model[docs], vector_size=100, sg=1, hs= 1, seed=33)\n",
    "bigram_counter = Counter()\n",
    "\n",
    "for key in w2vmodel.wv.key_to_index.keys(): #deprecated: w2vmodel.wv.vocab.keys()\n",
    "    if key not in stoplist:\n",
    "        if len(str(key).split(\"_\")) > 1:\n",
    "            bigram_counter[key] += w2vmodel.wv.get_vecattr(key, \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_mining -> ->  833\n",
      "voice_recognition -> ->  470\n",
      "speech_recognition -> ->  426\n",
      "machine_translation -> ->  202\n",
      "face_recognition -> ->  195\n",
      "computer_vision -> ->  140\n",
      "language_processing -> ->  103\n",
      "pattern_recognition -> ->  103\n",
      "forex_autopilot -> ->  66\n",
      "machine_learning -> ->  58\n",
      "supercomputer_center -> ->  50\n",
      "search_engine -> ->  48\n",
      "sentiment_analysis -> ->  38\n",
      "difficulty_level -> ->  23\n",
      "cash_formula -> ->  21\n",
      "post_location -> ->  20\n",
      "decision_support -> ->  19\n",
      "checkpoint_ccse -> ->  18\n",
      "join_date -> ->  18\n",
      "information_retrieval -> ->  18\n",
      "knowledge_discovery -> ->  17\n",
      "image_processing -> ->  17\n",
      "credit_card -> ->  16\n",
      "review_forex -> ->  14\n",
      "health_care -> ->  14\n",
      "bbeyonce_girl -> ->  14\n",
      "command_station -> ->  13\n",
      "currency_trading -> ->  12\n",
      "megadroid_review -> ->  12\n",
      "olap_cube -> ->  11\n",
      "university_california -> ->  11\n",
      "flight_simulator -> ->  11\n",
      "reality_wrestling -> ->  11\n"
     ]
    }
   ],
   "source": [
    "# get n most common bigrams\n",
    "for key, counts in bigram_counter.most_common(50):\n",
    "    print(key,\"-> -> \" ,counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjacent words, Example for network graph\n",
    "# w2vmodel.wv.most_similar(positive=['climate_change'], topn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens (before filter): 11215\n",
      "Number of unique tokens (after filter): 802\n",
      "Number of documents: 15541\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary and corpus for input to our LDA model\n",
    "# Filter out the most common and uncommon words\n",
    "dictionary = Dictionary(data_words_bigrams)\n",
    "print('Number of unique tokens (before filter): %d' % len(dictionary))\n",
    "\n",
    "# Filter out words that occur less than x documents, or more than y% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens (after filter): %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for lda multicore\n",
    "num_topics = 5\n",
    "passes = 150 # Number of passes through the corpus during training.\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for 150  passes:  560.2534658908844  seconds\n"
     ]
    }
   ],
   "source": [
    "# train LDA model/topic model\n",
    "t0 = time.time()\n",
    "ldamodel = LdaMulticore(corpus,\n",
    "                        id2word=dictionary,\n",
    "                        num_topics=num_topics,\n",
    "                        alpha='asymmetric',\n",
    "                        chunksize= 4000,\n",
    "                        batch= True,\n",
    "                        minimum_probability=0.001,\n",
    "                        iterations=350,\n",
    "                        passes=passes\n",
    "                        )\n",
    "\n",
    "t1= time.time()\n",
    "print(\"time for\",passes,\" passes: \",(t1-t0),\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_topics = {}\n",
    "d_topics_clean = {}\n",
    "\n",
    "for i in range(0, num_topics):\n",
    "    d_topics[i] = ldamodel.show_topics(num_words=10, formatted=False)[i][1]\n",
    "#for i in d:\n",
    "#[item[0] for item in second_topic]\n",
    "for key in d_topics:\n",
    "    list(d_topics.values())[key]\n",
    "    d_topics_clean[key] = [item[0] for item in list(d_topics.values())[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in d_topics_clean:\n",
    "    # print(list(d_topics_clean.values())[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store major topic\n",
    "#lda_corpus = ldamodel[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main topic for all reviews\n",
    "all_topics = ldamodel.get_document_topics(corpus)\n",
    "num_docs = len(all_topics)\n",
    "\n",
    "all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "\n",
    "major_topic = [np.argmax(arr) for arr in all_topics_numpy]\n",
    "df_orig['major_lda_topic'] = major_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of topics in statements\n",
    "\n",
    "sns.set(rc= {'figure.figsize': (20,5)})\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "df_orig.major_lda_topic.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer, face, people, recognition, software, speech, system, technology, time, voice\n",
      "\n",
      "change, enemy, file, game, level, network, player, point, team, time\n",
      "\n",
      "analysis, analytics, data, information, mining, research, supercomputer, system, test, tool\n",
      "\n",
      "autopilot, business, flight, forex, money, review, system, time, trading, unit\n",
      "\n",
      "computer, human, intelligence, language, machine, processing, science, technology, translation, world\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywords_openai = {}\n",
    "for key in d_topics_clean:\n",
    "    #keywords_string = ', '.join(sorted(list(d_topics_clean.values())[key]))\n",
    "    keywords_string = ''\n",
    "    for index, item in enumerate(sorted(list(d_topics_clean.values())[key])):\n",
    "        if index < len(list(d_topics_clean.values())[key])-1:\n",
    "            keywords_string = keywords_string + item + ', '\n",
    "        else:\n",
    "            keywords_string = keywords_string + item\n",
    "    print(keywords_string + '\\n')\n",
    "    keywords_openai[key] = keywords_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersect of topic 0 [10] and topic 1 [10]: 1\n",
      "-> ['time']\n",
      "Intersect of topic 0 [10] and topic 2 [10]: 1\n",
      "-> ['system']\n",
      "Intersect of topic 0 [10] and topic 3 [10]: 2\n",
      "-> ['system', 'time']\n",
      "Intersect of topic 0 [10] and topic 4 [10]: 2\n",
      "-> ['computer', 'technology']\n",
      "Intersect of topic 1 [10] and topic 2 [10]: 0\n",
      "-> []\n",
      "Intersect of topic 1 [10] and topic 3 [10]: 1\n",
      "-> ['time']\n",
      "Intersect of topic 1 [10] and topic 4 [10]: 0\n",
      "-> []\n",
      "Intersect of topic 2 [10] and topic 3 [10]: 1\n",
      "-> ['system']\n",
      "Intersect of topic 2 [10] and topic 4 [10]: 0\n",
      "-> []\n",
      "Intersect of topic 3 [10] and topic 4 [10]: 0\n",
      "-> []\n"
     ]
    }
   ],
   "source": [
    "for key_one in range(0, (len(d_topics_clean)-1)):\n",
    "    for key_two in range(key_one+1, len(d_topics_clean)):\n",
    "        inter = intersection(sorted(d_topics_clean[key_one]), sorted(d_topics_clean[key_two]))\n",
    "        print('Intersect of topic %d [%d] and topic %d [%d]: %d'%(key_one, len(d_topics_clean[key_one]), (key_two), len(d_topics_clean[key_two]), len(inter)))\n",
    "        print('-> %s'%inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face recognition\n",
      "(computer, face, people, recognition, software, speech, system, technology, time, voice)\n",
      "----\n",
      "New level\n",
      "(change, enemy, file, game, level, network, player, point, team, time)\n",
      "----\n",
      "analytics software\n",
      "(analysis, analytics, data, information, mining, research, supercomputer, system, test, tool)\n",
      "----\n",
      "Forex trading\n",
      "(autopilot, business, flight, forex, money, review, system, time, trading, unit)\n",
      "----\n",
      "The future of computing.\n",
      "(computer, human, intelligence, language, machine, processing, science, technology, translation, world)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "d_topics_subtopics = {}\n",
    "\n",
    "for key in keywords_openai:\n",
    "    response = openai.Completion.create(\n",
    "        model = \"text-davinci-002\",\n",
    "        prompt = \"One Topic of a maximum of two words for the following keywords: %s\"%list(keywords_openai.values())[key],\n",
    "        temperature = 0.7,\n",
    "        max_tokens = 256,\n",
    "        top_p = 1,\n",
    "        frequency_penalty = 0,\n",
    "        presence_penalty = 0,\n",
    "        stop=[\"\\\"\\\"\\\"\"]\n",
    "    )\n",
    "    response_loaded = response['choices'][0]['text']\n",
    "    response_list = []\n",
    "\n",
    "    #remove_pattern_1 = r'\\\\n[1234567890].'\n",
    "    #remove_pattern_1 = r'[\\w][\\d].'\n",
    "    remove_pattern_1 = r'[\\d].'\n",
    "    remove_pattern_2 = r'\\n'\n",
    "\n",
    "    response_loaded = re.sub(remove_pattern_1, ',', response_loaded)\n",
    "    response_loaded = re.sub(remove_pattern_2, '', response_loaded)\n",
    "    response_loaded = response_loaded.split(', ')\n",
    "    response_loaded = list(filter(None, response_loaded))\n",
    "\n",
    "    for i in response_loaded:\n",
    "        print(i)\n",
    "    print('(%s)'%list(keywords_openai.values())[key])\n",
    "    print('----')\n",
    "    d_topics_subtopics[response_loaded[0]] = list(d_topics_clean.values())[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So many verbs and adjectives -> remove before LdaMulticore modeling?\n",
    "# -> only nouns can be applied in section: '# Filter out words ...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_string = 'a,b,c'\n",
    "# letter_list = a_string.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_orig.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def major_lda_topic_to_topic(row, d_topics_subtopics):\n",
    "    for i in range(0, len(d_topics_subtopics)):\n",
    "        if row['major_lda_topic'] == i:\n",
    "            return list(d_topics_subtopics.keys())[i]\n",
    "    # if row['major_lda_topic'] == 0:\n",
    "    #     return list(d_topics_subtopics.keys())[0]\n",
    "    # elif row['major_lda_topic'] == 1:\n",
    "    #     return list(d_topics_subtopics.keys())[1]\n",
    "    # elif row['major_lda_topic'] == 2:\n",
    "    #     return list(d_topics_subtopics.keys())[2]\n",
    "    # elif row['major_lda_topic'] == 3:\n",
    "    #     return list(d_topics_subtopics.keys())[3]\n",
    "    # elif row['major_lda_topic'] == 4:\n",
    "    #     return list(d_topics_subtopics.keys())[4]\n",
    "\n",
    "def topic_to_subtopic(row, d_topics_subtopics):\n",
    "    for ii in range(0, len(d_topics_subtopics)):\n",
    "        if row['major_lda_topic'] == ii:\n",
    "            for i in list(d_topics_subtopics.values())[ii]:\n",
    "                #if row['statement'].str.contains(i, case=False, na=False):\n",
    "                if re.search(i, row['statement'], re.IGNORECASE):\n",
    "                    return i\n",
    "            return 'undefined'\n",
    "    # if row['major_lda_topic'] == 0:\n",
    "    #     for i in list(d_topics_subtopics.values())[0]:\n",
    "    #         #if row['statement'].str.contains(i, case=False, na=False):\n",
    "    #         if re.search(i, row['statement'], re.IGNORECASE):\n",
    "    #             return i\n",
    "    #     return 'undefined'\n",
    "    # elif row['major_lda_topic'] == 1:\n",
    "    #     for i in list(d_topics_subtopics.values())[1]:\n",
    "    #         #if row['statement'].str.contains(i, case=False, na=False):\n",
    "    #         if re.search(i, row['statement'], re.IGNORECASE):\n",
    "    #             return i\n",
    "    #     return 'undefined'\n",
    "    # elif row['major_lda_topic'] == 2:\n",
    "    #     for i in list(d_topics_subtopics.values())[2]:\n",
    "    #         #if row['statement'].str.contains(i, case=False, na=False):\n",
    "    #         if re.search(i, row['statement'], re.IGNORECASE):\n",
    "    #             return i\n",
    "    #     return 'undefined'\n",
    "    # elif row['major_lda_topic'] == 3:\n",
    "    #     for i in list(d_topics_subtopics.values())[3]:\n",
    "    #         #if row['statement'].str.contains(i, case=False, na=False):\n",
    "    #         if re.search(i, row['statement'], re.IGNORECASE):\n",
    "    #             return i\n",
    "    #     return 'undefined'\n",
    "    # elif row['major_lda_topic'] == 4:\n",
    "    #     for i in list(d_topics_subtopics.values())[4]:\n",
    "    #         #if row['statement'].str.contains(i, case=False, na=False):\n",
    "    #         if re.search(i, row['statement'], re.IGNORECASE):\n",
    "    #             return i\n",
    "    #     return 'undefined'\n",
    "\n",
    "def subtopic_to_network(row, d_topics_subtopics_merged):\n",
    "    network = []\n",
    "    for index, item in enumerate(d_topics_subtopics_merged):\n",
    "        if (re.search(item, row['statement'], re.IGNORECASE)) and (item.casefold() not in network):\n",
    "            network.append(item)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig['topic'] = df_orig.apply(lambda row: major_lda_topic_to_topic(row, d_topics_subtopics), axis=1)\n",
    "df_orig['subtopic'] = df_orig.apply(lambda row: topic_to_subtopic(row, d_topics_subtopics), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_topics_subtopics_merged = []\n",
    "for i in range(0,len(d_topics_subtopics)):\n",
    "    d_topics_subtopics_merged += list(d_topics_subtopics.values())[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig['network'] = df_orig.apply(lambda row: subtopic_to_network(row, d_topics_subtopics_merged), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "Face recognition            5483\n",
       "Forex trading               1764\n",
       "New level                   3305\n",
       "The future of computing.    1672\n",
       "analytics software          3317\n",
       "dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.groupby(['topic']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic                     subtopic     \n",
       "Face recognition          computer          340\n",
       "                          face               59\n",
       "                          people            130\n",
       "                          recognition      1508\n",
       "                          software           78\n",
       "                          speech              5\n",
       "                          system            306\n",
       "                          technology         61\n",
       "                          time              139\n",
       "                          undefined        2843\n",
       "                          voice              14\n",
       "Forex trading             autopilot        1001\n",
       "                          business            8\n",
       "                          flight             26\n",
       "                          forex               2\n",
       "                          money              18\n",
       "                          review             11\n",
       "                          system             48\n",
       "                          time               43\n",
       "                          trading             9\n",
       "                          undefined         530\n",
       "                          unit               68\n",
       "New level                 change             67\n",
       "                          enemy              58\n",
       "                          file               79\n",
       "                          game              648\n",
       "                          level             104\n",
       "                          network           326\n",
       "                          player            316\n",
       "                          point              85\n",
       "                          team               72\n",
       "                          time              235\n",
       "                          undefined        1315\n",
       "The future of computing.  computer           56\n",
       "                          human              44\n",
       "                          intelligence      752\n",
       "                          language          101\n",
       "                          machine           263\n",
       "                          processing          7\n",
       "                          science            91\n",
       "                          technology         21\n",
       "                          translation         1\n",
       "                          undefined         317\n",
       "                          world              19\n",
       "analytics software        analysis           47\n",
       "                          analytics          64\n",
       "                          data             1898\n",
       "                          information        51\n",
       "                          mining              6\n",
       "                          research          120\n",
       "                          supercomputer     382\n",
       "                          system             58\n",
       "                          test              100\n",
       "                          tool               29\n",
       "                          undefined         562\n",
       "dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.groupby(['topic','subtopic']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chop substring at the end of string\n",
    "def rchop(s, suffix):\n",
    "    if suffix and s.endswith(suffix):\n",
    "        return s[:-len(suffix)]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataframe to .csv\n",
    "path = rchop(path, '.csv')\n",
    "path_plus_topic = path + '_altered' + '.csv'\n",
    "\n",
    "df_orig.to_csv(path_plus_topic, sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
