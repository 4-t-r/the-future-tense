{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /mnt/ceph/storage/data-\n",
      "[nltk_data]     tmp/teaching-current//aq60qovu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /mnt/ceph/storage/data-\n",
      "[nltk_data]     tmp/teaching-current//aq60qovu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /mnt/ceph/storage/data-tmp/teaching-\n",
      "[nltk_data]     current//aq60qovu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /mnt/ceph/storage/data-\n",
      "[nltk_data]     tmp/teaching-current//aq60qovu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /mnt/ceph/storage/data-\n",
      "[nltk_data]     tmp/teaching-current//aq60qovu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import transformers\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#import torch\n",
    "#from huggingface_hub import notebook_login\n",
    "#from transformers import pipeline\n",
    "#from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stoplist=stopwords.words('english')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer= WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import gensim\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing gensim related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['statement', 'future', 'source'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig = pd.read_csv('../../datasets/future_statements_dataset/future_dataset.csv', sep='|')\n",
    "df_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is used to get the part-of-speech(POS) for lemmatization\n",
    "\"\"\"\n",
    "def get_tags(tag):\n",
    "    if tag.startswith('N') or tag.startswith('J'):\n",
    "       return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "      return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "      return wordnet.ADV\n",
    "    else:\n",
    "      return wordnet.NOUN #default case\n",
    "\n",
    "\"\"\"\n",
    "1. Removes Punctuations\n",
    "2. Removes words smaller than 3 letters\n",
    "3. Converts into lowercase\n",
    "4. Lemmatizes words\n",
    "5. Removes Stopwords\n",
    "\"\"\"\n",
    "def preprocess(text):\n",
    "    punctuation= list(string.punctuation)\n",
    "    doc_tokens= nltk.word_tokenize(text)\n",
    "    word_tokens= [word.lower() for word in doc_tokens if not (word in punctuation or len(word)<=3)]\n",
    "    # Lemmatize\n",
    "    pos_tags=nltk.pos_tag(word_tokens)\n",
    "    doc_words=[wordnet_lemmatizer.lemmatize(word, pos=get_tags(tag)) for word, tag in pos_tags]\n",
    "    doc_words= [word for word in doc_words if word not in stoplist]\n",
    "    return doc_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [need, rededicate, principle, purpose, organiz...\n",
       "1     [many, thing, leave, behind, many, thing, await]\n",
       "2    [philippine, stand, ready, assist, implement, ...\n",
       "3    [first, indigenous, population, paraguay, larg...\n",
       "4    [also, grateful, principled, position, home, m...\n",
       "Name: statement, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_orig['statement'].apply(preprocess)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= list(df_clean)\n",
    "phrases = gensim.models.Phrases(docs, min_count=10, threshold=20)\n",
    "bigram_model = gensim.models.phrases.Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create bigrams from statements\n",
    "'''\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_model[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "united_nation -> 100\n",
      "international_community -> 38\n",
      "security_council -> 31\n",
      "general_assembly -> 29\n",
      "unite_nation -> 24\n",
      "develop_country -> 22\n",
      "next_year -> 20\n",
      "climate_change -> 19\n",
      "last_year -> 19\n",
      "peace_security -> 17\n",
      "past_year -> 14\n",
      "human_right -> 14\n",
      "united_state -> 13\n",
      "first_time -> 12\n",
      "take_place -> 12\n",
      "elon_musk -> 12\n",
      "supply_chain -> 12\n",
      "council_resolution -> 11\n",
      "middle_east -> 10\n",
      "five_year -> 10\n"
     ]
    }
   ],
   "source": [
    "# Checkout most frequent bigrams\n",
    "bigram_counter1= Counter()\n",
    "for key in phrases.vocab.keys():\n",
    "    if key not in stopwords.words('english'):\n",
    "        if len(str(key).split('_'))>1:\n",
    "            bigram_counter1[key]+=phrases.vocab[key]\n",
    "\n",
    "for key, counts in bigram_counter1.most_common(20):\n",
    "    print(key,\"->\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelstep: Feeding the bigrams into a Word2Vec model produces more meaningful bigrams\n",
    "w2vmodel = Word2Vec(sentences=bigram_model[docs], vector_size=100, sg=1, hs= 1, seed=33)\n",
    "bigram_counter = Counter()\n",
    "\n",
    "for key in w2vmodel.wv.key_to_index.keys(): #deprecated: w2vmodel.wv.vocab.keys()\n",
    "    if key not in stoplist:\n",
    "        if len(str(key).split(\"_\")) > 1:\n",
    "            bigram_counter[key] += w2vmodel.wv.get_vecattr(key, \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "united_nation -> ->  100\n",
      "international_community -> ->  38\n",
      "security_council -> ->  31\n",
      "general_assembly -> ->  29\n",
      "unite_nation -> ->  24\n",
      "develop_country -> ->  22\n",
      "next_year -> ->  20\n",
      "climate_change -> ->  19\n",
      "last_year -> ->  19\n",
      "peace_security -> ->  17\n",
      "human_right -> ->  14\n",
      "supply_chain -> ->  12\n",
      "elon_musk -> ->  12\n"
     ]
    }
   ],
   "source": [
    "# get n most common bigrams\n",
    "for key, counts in bigram_counter.most_common(50):\n",
    "    print(key,\"-> -> \" ,counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjacent words, Example for network graph\n",
    "# w2vmodel.wv.most_similar(positive=['climate_change'], topn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens (before filter): 6302\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary and corpus for input to our LDA model\n",
    "# Filter out the most common and uncommon words\n",
    "dictionary= Dictionary(data_words_bigrams)\n",
    "print('Number of unique tokens (before filter): %d' % len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens (after filter): 274\n",
      "Number of documents: 2500\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur less than x documents, or more than y% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens (after filter): %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for 150  passes:  378.6969721317291  seconds\n"
     ]
    }
   ],
   "source": [
    "# train LDA model/topic model\n",
    "t0= time.time()\n",
    "passes= 150\n",
    "np.random.seed(1) # setting up random seed to get the same results\n",
    "num_topics = 4\n",
    "ldamodel= LdaMulticore(corpus,\n",
    "                        id2word=dictionary,\n",
    "                        num_topics=num_topics,\n",
    "                        alpha='asymmetric',\n",
    "                        chunksize= 4000,\n",
    "                        batch= True,\n",
    "                        minimum_probability=0.001,\n",
    "                        iterations=350,\n",
    "                        passes=passes\n",
    "                        )\n",
    "\n",
    "t1= time.time()\n",
    "print(\"time for\",passes,\" passes: \",(t1-t0),\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people',\n",
       " 'country',\n",
       " 'world',\n",
       " 'problem',\n",
       " 'economic',\n",
       " 'peace',\n",
       " 'life',\n",
       " 'security',\n",
       " 'development',\n",
       " 'develop',\n",
       " 'issue',\n",
       " 'international',\n",
       " 'make',\n",
       " 'social',\n",
       " 'government',\n",
       " 'political',\n",
       " 'human',\n",
       " 'achieve',\n",
       " 'know',\n",
       " 'important',\n",
       " 'force',\n",
       " 'progress',\n",
       " 'region',\n",
       " 'support',\n",
       " 'solution']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_topic = ldamodel.show_topics(num_words=25, formatted=False)[0][1]\n",
    "second_topic = ldamodel.show_topics(num_words=25, formatted=False)[1][1]\n",
    "third_topic = ldamodel.show_topics(num_words=25, formatted=False)[2][1]\n",
    "forth_topic = ldamodel.show_topics(num_words=25, formatted=False)[3][1]\n",
    "[item[0] for item in second_topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store major topic\n",
    "lda_corpus= ldamodel[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main topic for all reviews\n",
    "all_topics = ldamodel.get_document_topics(corpus)\n",
    "num_docs = len(all_topics)\n",
    "\n",
    "all_topics_csr= gensim.matutils.corpus2csc(all_topics)\n",
    "all_topics_numpy= all_topics_csr.T.toarray()\n",
    "\n",
    "major_topic= [np.argmax(arr) for arr in all_topics_numpy]\n",
    "df_orig['major_lda_topic']= major_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of topics in statements\n",
    "\n",
    "sns.set(rc= {'figure.figsize': (5,3)})\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "df_orig.major_lda_topic.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_openai = '''AI\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "model=\"text-davinci-002\",\n",
    "prompt=\"Topic (of maximum 2 Words) for these keywords: %s\"%keywords_openai,\n",
    "temperature=0.7,\n",
    "max_tokens=256,\n",
    "top_p=1,\n",
    "frequency_penalty=0,\n",
    "presence_penalty=0,\n",
    "stop=[\"\\\"\\\"\\\"\"]\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
