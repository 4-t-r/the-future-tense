{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8153dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48af4974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /mnt/ceph/storage/data-tmp/teaching-current//aq60qovu/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6767b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    #AutoTokenizer,\n",
    "    DistilBertTokenizerFast,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    PushToHubCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8aeeeb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(f'Loading data...')\n",
    "    # Load data as series\n",
    "    _X_train = \\\n",
    "        pd.read_csv(\n",
    "            '../../datasets/future_statements_dataset/X_train.csv'\n",
    "        )['statement']\n",
    "    _y_train = \\\n",
    "        pd.read_csv(\n",
    "            '../../datasets/future_statements_dataset/y_train.csv'\n",
    "        )['future']\n",
    "\n",
    "    # Create train/test split\n",
    "    _X_train, _X_test, _y_train, _y_test = \\\n",
    "        train_test_split(_X_train, _y_train, test_size=0.2)\n",
    "\n",
    "    # Create train/validation split\n",
    "    _X_train, _X_valid, _y_train, _y_valid = \\\n",
    "        train_test_split(_X_train, _y_train, test_size=0.2)\n",
    "\n",
    "    # Sort index\n",
    "    _X_train.sort_index(inplace=True)\n",
    "    _X_valid.sort_index(inplace=True)\n",
    "    _X_test.sort_index(inplace=True)\n",
    "    _y_train.sort_index(inplace=True)\n",
    "    _y_valid.sort_index(inplace=True)\n",
    "    _y_test.sort_index(inplace=True)\n",
    "\n",
    "    # Reset index\n",
    "    _X_train.reset_index(drop=True, inplace=True)\n",
    "    _X_valid.reset_index(drop=True, inplace=True)\n",
    "    _X_test.reset_index(drop=True, inplace=True)\n",
    "    _y_train.reset_index(drop=True, inplace=True)\n",
    "    _y_valid.reset_index(drop=True, inplace=True)\n",
    "    _y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print('Training data:   ', len(_X_train.index), ' rows.')\n",
    "    print('Validation data: ', len(_X_valid.index), ' rows.')\n",
    "    print('Test data:       ', len(_X_test.index), ' rows.')\n",
    "\n",
    "    print(f'Loading done...')\n",
    "    return _X_train.tolist(), _X_valid.tolist(), _X_test.tolist(), \\\n",
    "        _y_train.tolist(), _y_valid.tolist(), _y_test.tolist()\n",
    "\n",
    "def encode_data(dataset, _tokenizer):\n",
    "    print(f'Encoding data...')\n",
    "    inputs = _tokenizer(dataset,\n",
    "                        max_length=128,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_token_type_ids=False)\n",
    "    ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    print(f'Encoding done...')\n",
    "    return tf.convert_to_tensor(ids), tf.convert_to_tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3648fd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training data:    1600  rows.\n",
      "Validation data:  400  rows.\n",
      "Test data:        500  rows.\n",
      "Loading done...\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ec26f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 00:31:06.242344: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-17 00:31:06.370524: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-17 00:31:06.370676: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (betaweb036): /proc/driver/nvidia/version does not exist\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=2\n",
    "    )\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "    'distilbert-base-uncased'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d6a6a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding data...\n",
      "Encoding done...\n",
      "Encoding data...\n",
      "Encoding done...\n",
      "Encoding data...\n",
      "Encoding done...\n"
     ]
    }
   ],
   "source": [
    "# Get train_ids and attention_mask\n",
    "X_train_ids, X_train_attention_mask = encode_data(X_train, tokenizer)\n",
    "X_valid_ids, X_valid_attention_mask = encode_data(X_valid, tokenizer)\n",
    "X_test_ids, X_test_attention_mask = encode_data(X_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebf26154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  mode='min',\n",
    "                                                  min_delta=0,\n",
    "                                                  patience=0,\n",
    "                                                  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d70f1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/the-future-tense/src/future_model_ft/model_output is already a clone of https://huggingface.co/fidsinn/bert-fine-tuned-cola. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "push_cb = [PushToHubCallback(\"model_output/\", \n",
    "                               tokenizer=tokenizer,\n",
    "                               hub_model_id=\"bert-fine-tuned-cola\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ade21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 00:36:13.657920: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 100663296 exceeds 10% of free system memory.\n",
      "2022-08-17 00:36:14.373703: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 100663296 exceeds 10% of free system memory.\n",
      "2022-08-17 00:36:14.373822: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 100663296 exceeds 10% of free system memory.\n",
      "2022-08-17 00:36:14.466509: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 100663296 exceeds 10% of free system memory.\n",
      "2022-08-17 00:36:14.466608: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 100663296 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7/25 [=======>......................] - ETA: 13:12 - loss: 0.6772 - sparse_categorical_accuracy: 0.6629"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(\n",
    "    x=[X_train_ids, X_train_attention_mask],\n",
    "    y=np.asarray(y_train),\n",
    "    epochs=6,\n",
    "    batch_size=64,\n",
    "    steps_per_epoch=len(X_train) // 64,\n",
    "    validation_data=([X_valid_ids, X_valid_attention_mask],\n",
    "                     np.asarray(y_valid)),\n",
    "    callbacks=[early_stopping,\n",
    "              push_cb\n",
    "              ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb8535",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"distilbert-base-future\", commit_message=\"End of training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
