{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import transformers\n",
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stoplist=stopwords.words('english')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer= WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing gensim related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '../../datasets/test_dataset_model_pipeline/future_statements.csv'\n",
    "path = '../stage_1_warc_dl/warc_dl_output/future_statements.csv'\n",
    "\n",
    "df_orig = pd.read_csv(path, sep='|', error_bad_lines=False)\n",
    "df_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(tag):\n",
    "   \"\"\"\n",
    "   This function is used to get the part-of-speech(POS) for lemmatization\n",
    "   \"\"\"\n",
    "   if tag.startswith('N') or tag.startswith('J'):\n",
    "      return wordnet.NOUN\n",
    "   #elif tag.startswith('J'):\n",
    "   #   return wordnet.ADJ\n",
    "   elif tag.startswith('V'):\n",
    "      return wordnet.VERB\n",
    "   elif tag.startswith('R'):\n",
    "      return wordnet.ADV\n",
    "   else:\n",
    "      return wordnet.NOUN #default case\n",
    "\n",
    "def preprocess(text):\n",
    "   \"\"\"\n",
    "   1. Removes Punctuations\n",
    "   2. Removes words smaller than 3 letters\n",
    "   3. Converts into lowercase\n",
    "   4. Lemmatizes words\n",
    "   5. Removes Stopwords\n",
    "   \"\"\"\n",
    "   punctuation = list(string.punctuation)\n",
    "   doc_tokens = nltk.word_tokenize(text)\n",
    "   word_tokens = [word.lower() for word in doc_tokens if not (word in punctuation or len(word)<=3)]\n",
    "   # Lemmatize\n",
    "   _pos_tags = nltk.pos_tag(word_tokens)\n",
    "   pos_tags = []\n",
    "   for i in _pos_tags:\n",
    "      if re.search(r'(N)\\w+',i[1]):\n",
    "         pos_tags.append(i)\n",
    "   doc_words = [wordnet_lemmatizer.lemmatize(word, pos = get_tags(tag)) for word, tag in pos_tags]\n",
    "   doc_words = [word for word in doc_words if word not in stoplist]\n",
    "   return doc_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_orig['statement'].apply(preprocess)\n",
    "docs= list(df_clean)\n",
    "phrases = gensim.models.Phrases(docs, min_count=10, threshold=20)\n",
    "bigram_model = gensim.models.phrases.Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    '''\n",
    "    create bigrams from statements\n",
    "    '''\n",
    "    return [bigram_model[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(docs)\n",
    "# Checkout most frequent bigrams\n",
    "bigram_counter1 = Counter()\n",
    "for key in phrases.vocab.keys():\n",
    "    if key not in stopwords.words('english'):\n",
    "        if len(str(key).split('_'))>1:\n",
    "            bigram_counter1[key]+=phrases.vocab[key]\n",
    "#present most common bigrams\n",
    "for key, counts in bigram_counter1.most_common(15):\n",
    "    print(key,\"->\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2vmodel_create(bigram_model, docs, stoplist):\n",
    "    '''\n",
    "    modelstep: Feeding the bigrams into a Word2Vec model produces more meaningful bigrams\n",
    "    '''\n",
    "    w2vmodel = Word2Vec(sentences=bigram_model[docs], vector_size=100, sg=1, hs= 1)\n",
    "    bigram_counter = Counter()\n",
    "\n",
    "    for key in w2vmodel.wv.key_to_index.keys():\n",
    "        if key not in stoplist:\n",
    "            if len(str(key).split(\"_\")) > 1:\n",
    "                bigram_counter[key] += w2vmodel.wv.get_vecattr(key, \"count\")\n",
    "    return w2vmodel, bigram_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel, bigram_counter = w2vmodel_create(bigram_model, docs, stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n most common bigrams\n",
    "for key, counts in bigram_counter.most_common(15):\n",
    "    print(key,\"-> -> \" ,counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_and_corpus(data_words_bigrams, docs):\n",
    "    '''\n",
    "    Create a dictionary and corpus for input to our LDA model\n",
    "    Filter out the most common and uncommon words\n",
    "    '''\n",
    "    dictionary = Dictionary(data_words_bigrams)\n",
    "    print('Number of unique tokens (before filter): %d' % len(dictionary))\n",
    "\n",
    "    # Filter out words that occur less than x documents, or more than y% of the documents.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    print('Number of unique tokens (after filter): %d' % len(dictionary))\n",
    "    print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, corpus = create_dict_and_corpus(data_words_bigrams, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ldamodel(dictionary, num_topics, passes, ldamodel_file, load_model):\n",
    "    '''\n",
    "    create or load LDA model/topic model\n",
    "    '''\n",
    "    if load_model and os.path.exists(os.path.join(ldamodel_file, 'lda')):\n",
    "        print('Load model...')\n",
    "        ldamodel = LdaModel.load(os.path.join(ldamodel_file, 'lda'))\n",
    "    else:\n",
    "        print('Create model...')\n",
    "        if os.path.exists(ldamodel_file):\n",
    "            shutil.rmtree(ldamodel_file)\n",
    "        os.mkdir(ldamodel_file)\n",
    "        t0 = time.time()\n",
    "        ldamodel = LdaMulticore(corpus,\n",
    "                                id2word=dictionary,\n",
    "                                num_topics=num_topics,\n",
    "                                alpha='asymmetric',\n",
    "                                chunksize= 4000,\n",
    "                                batch= True,\n",
    "                                minimum_probability=0.001,\n",
    "                                iterations=350,\n",
    "                                passes=passes\n",
    "                                )\n",
    "        ldamodel.save(os.path.join(ldamodel_file, 'lda'))\n",
    "        t1= time.time()\n",
    "        print(\"...time for\",passes,\" passes: \",(t1-t0),\" seconds\")\n",
    "    return ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for lda multicore\n",
    "num_topics = 10\n",
    "passes = 150 # Number of passes through the corpus during training.\n",
    "np.random.seed(1)\n",
    "\n",
    "ldamodel_file = \"ldamodel\"\n",
    "ldamodel = create_ldamodel(dictionary, num_topics, passes, ldamodel_file, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_subtopics_dict():\n",
    "    '''\n",
    "    create clean topic dictionaries with topic as key and subtopics as values\n",
    "    '''\n",
    "    d_topics = {}\n",
    "    d_topics_clean = {}\n",
    "    for i in range(0, num_topics):\n",
    "        d_topics[i] = ldamodel.show_topics(num_words=20, formatted=False)[i][1]\n",
    "    #for i in d:\n",
    "    #[item[0] for item in second_topic]\n",
    "    for key in d_topics:\n",
    "        list(d_topics.values())[key]\n",
    "        d_topics_clean[key] = [item[0] for item in list(d_topics.values())[key]]\n",
    "    return d_topics, d_topics_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_topics = {}\n",
    "d_topics_clean = {}\n",
    "d_topics, d_topics_clean = topics_subtopics_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_major_lda_topic(ldamodel, corpus, df_orig):\n",
    "    '''\n",
    "    set main topic for all reviews\n",
    "    '''\n",
    "    all_topics = ldamodel.get_document_topics(corpus)\n",
    "    num_docs = len(all_topics)\n",
    "\n",
    "    all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n",
    "    all_topics_numpy = all_topics_csr.T.toarray()\n",
    "\n",
    "    major_topic = [np.argmax(arr) for arr in all_topics_numpy]\n",
    "    df_orig['major_lda_topic'] = major_topic\n",
    "    return df_orig['major_lda_topic']\n",
    "\n",
    "def plot_topics_dist(df_orig):\n",
    "    '''\n",
    "    plot distribution of topics in statements\n",
    "    '''\n",
    "    sns.set(rc= {'figure.figsize': (20,5)})\n",
    "    sns.set_style('darkgrid')\n",
    "    df_orig['major_lda_topic'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig['major_lda_topic'] = set_major_lda_topic(ldamodel, corpus, df_orig)\n",
    "plot_topics_dist(df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keywords_openai(d_topics_clean):\n",
    "    '''\n",
    "    create keywords to use in openai prompt\n",
    "    '''\n",
    "    keywords_openai = {}\n",
    "    for key in d_topics_clean:\n",
    "        #keywords_string = ', '.join(sorted(list(d_topics_clean.values())[key]))\n",
    "        keywords_string = ''\n",
    "        for index, item in enumerate(list(d_topics_clean.values())[key]):\n",
    "            if index < len(list(d_topics_clean.values())[key])-1:\n",
    "                keywords_string = keywords_string + item + ', '\n",
    "            else:\n",
    "                keywords_string = keywords_string + item\n",
    "        keywords_openai[key] = keywords_string\n",
    "    return keywords_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_openai = {}\n",
    "keywords_openai = create_keywords_openai(d_topics_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    '''\n",
    "    calculate amount of intersection\n",
    "    '''\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def eval_intersections(d_topics_clean):\n",
    "    '''\n",
    "    evaluate intersections between topic clusters\n",
    "    '''\n",
    "    for key_one in range(0, (len(d_topics_clean)-1)):\n",
    "        for key_two in range(key_one+1, len(d_topics_clean)):\n",
    "            inter = intersection(sorted(d_topics_clean[key_one]), sorted(d_topics_clean[key_two]))\n",
    "            print('Intersect of topic %d [%d] and topic %d [%d]: %d'%(key_one, len(d_topics_clean[key_one]), (key_two), len(d_topics_clean[key_two]), len(inter)))\n",
    "            print('-> %s'%inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_intersections(d_topics_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_name_generator(keywords_openai, d_topics_clean, oai_api_key):\n",
    "    '''\n",
    "    generate topic names with openai's text-davinci-002, based on the subtopics that are clustered by lda\n",
    "    '''\n",
    "    openai.api_key = oai_api_key\n",
    "\n",
    "    d_topics_subtopics = {}\n",
    "\n",
    "    for key in keywords_openai:\n",
    "        response = openai.Completion.create(\n",
    "            model = \"text-davinci-002\",\n",
    "            prompt = \"One topic of a maximum of two words for the following keywords without using the keywords: %s\"%list(keywords_openai.values())[key],\n",
    "            # prompt = \"\"\"Extract categories from this list:\n",
    "            # %s\n",
    "            # \"\"\"%list(keywords_openai.values())[key],\n",
    "            #prompt = \"Best matching category of a maximum of two words for the following keywords %s\"%list(keywords_openai.values())[key],\n",
    "            #prompt = \"One best matching headline of a maximum of two words like 'word1 word2' for the following keywords %s\"%list(keywords_openai.values())[key],\n",
    "            temperature=0.3,\n",
    "            max_tokens=256,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.8,\n",
    "            presence_penalty=0.0,\n",
    "            stop=[\"\\\"\\\"\\\"\"]\n",
    "        )\n",
    "        response_loaded = response['choices'][0]['text']\n",
    "        response_list = []\n",
    "\n",
    "        remove_pattern_1 = r'[\\d].'\n",
    "        remove_pattern_2 = r'\\n'\n",
    "\n",
    "        response_loaded = re.sub(remove_pattern_1, ',', response_loaded)\n",
    "        response_loaded = re.sub(remove_pattern_2, '', response_loaded)\n",
    "        response_loaded = response_loaded.split(', ')\n",
    "        response_loaded = list(filter(None, response_loaded))\n",
    "\n",
    "        for i in response_loaded:\n",
    "            print('--', i)\n",
    "        print('(%s)'%list(keywords_openai.values())[key])\n",
    "        print('-------------')\n",
    "        print('-------------')\n",
    "        d_topics_subtopics[response_loaded[0]] = list(d_topics_clean.values())[key]\n",
    "    return d_topics_subtopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_api_key = os.getenv(\"OPENAI_API_KEY\") #add openai_api_key here or use environment variable\n",
    "d_topics_subtopics = topic_name_generator(keywords_openai, d_topics_clean, oai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_subtopics_as_list(d_topics_subtopics):\n",
    "    d_topics_subtopics_merged = []\n",
    "    for i in range(0,len(d_topics_subtopics)):\n",
    "        d_topics_subtopics_merged += list(d_topics_subtopics.values())[i]\n",
    "\n",
    "    with open(r'subtopics.txt', 'w') as fp:\n",
    "        fp.write(','.join(d_topics_subtopics_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_subtopics_as_list(d_topics_subtopics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
