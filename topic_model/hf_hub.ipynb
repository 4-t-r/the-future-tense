{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad8e3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 09:38:49.449481: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-07 09:38:49.449589: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import random\n",
    "import scikitplot as skplt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8c3b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../datasets/future_statements_dataset/X_train.csv')[\"statement\"]\n",
    "y_train = pd.read_csv('../datasets/future_statements_dataset/y_train.csv')[\"future\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d12b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'MAX_LENGTH': 128,\n",
    "          'EPOCHS': 50,\n",
    "          #learningrate\n",
    "          'LEARNING_RATE': 5e-5,\n",
    "          'FT_EPOCHS': 10,\n",
    "          'OPTIMIZER': 'adam',\n",
    "          'FL_GAMMA': 2.0,\n",
    "          'FL_ALPHA': 0.2,\n",
    "          'BATCH_SIZE': 64,\n",
    "          'NUM_STEPS': len(X_train.index) // 64,\n",
    "          #dropouts:\n",
    "          'DISTILBERT_DROPOUT': 0.2,\n",
    "          'DISTILBERT_ATT_DROPOUT': 0.2,\n",
    "          'LAYER_DROPOUT': 0.2,\n",
    "          'KERNEL_INITIALIZER': 'GlorotNormal',\n",
    "          'BIAS_INITIALIZER': 'zeros',\n",
    "          'POS_PROBA_THRESHOLD': 0.5,\n",
    "          'ADDED_LAYERS': 'Dense 256, Dense 32, Dropout 0.2',\n",
    "          'LR_SCHEDULE': '5e-5 for 6 epochs, Fine-tune w/ adam for 2 epochs @2e-5',\n",
    "          'FREEZING': 'All DistilBERT layers frozen for 6 epochs, then unfrozen for 2',\n",
    "          'CALLBACKS': '[early_stopping w/ patience=0]',\n",
    "          'RANDOM_STATE': 42\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68b1e103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_77', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e579f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples[\"statement\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ca1d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode(_tokenizer, texts, batch_size=256, max_length=params['MAX_LENGTH']):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    A function that encodes a batch of texts and returns the texts'\n",
    "    corresponding encodings and attention masks that are ready to be fed\n",
    "    into a pre-trained transformer model.\n",
    "    Input:\n",
    "        - _tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
    "        - texts:       List of strings where each string represents a text\n",
    "        - batch_size:  Integer controlling number of texts in a batch\n",
    "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
    "    Output:\n",
    "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
    "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = _tokenizer.batch_encode_plus(batch,\n",
    "                                              max_length=max_length,\n",
    "                                              padding='max_length',\n",
    "                                              truncation=True,\n",
    "                                              return_attention_mask=True,\n",
    "                                              return_token_type_ids=False\n",
    "                                              )\n",
    "        input_ids.extend(inputs['input_ids'])\n",
    "        attention_mask.extend(inputs['attention_mask'])\n",
    "\n",
    "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47717e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafa25c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2057, 2342, 2000, 2417, 2098, 24695, 9731, 2000, 1996, 6481, 1998, 5682, 2005, 2029, 2023, 3029, 2001, 2631, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a469827c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We need to rededicate ourselves to the principles and purposes for which this Organization was founded.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17377fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "# Create train/validation split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aafbf35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:    1600  rows. Negatives: 808 Positives: 792\n",
      "Validation data:  400  rows. Negatives: 197 Positives: 203\n",
      "Test data:        500  rows. Negatives: 245 Positives: 255\n"
     ]
    }
   ],
   "source": [
    "# Sort index\n",
    "X_train.sort_index(inplace=True)\n",
    "X_valid.sort_index(inplace=True)\n",
    "X_test.sort_index(inplace=True)\n",
    "y_train.sort_index(inplace=True)\n",
    "y_valid.sort_index(inplace=True)\n",
    "y_test.sort_index(inplace=True)\n",
    "\n",
    "# Reset index\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_valid.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_valid.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('Training data:   ', len(X_train.index), ' rows. Negatives:', (y_train==0).sum(), 'Positives:', (y_train==1).sum())\n",
    "print('Validation data: ', len(X_valid.index), ' rows. Negatives:', (y_valid==0).sum(), 'Positives:', (y_valid==1).sum())\n",
    "print('Test data:       ', len(X_test.index), ' rows. Negatives:', (y_test==0).sum(), 'Positives:', (y_test==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e1a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode X_train\n",
    "X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n",
    "\n",
    "# Encode X_valid\n",
    "X_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist())\n",
    "\n",
    "# Encode X_test\n",
    "X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80a1ef32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1600, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b0d6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_short = X_train[[1, 2, 501, 502]]\n",
    "y_train_short = y_train[[1, 2, 501, 502]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb81a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2034, 1010, 1996, 6284, 2313, 1011, 2029, 1999, 13884, 2003, 2025, 2312, 1010, 2561, 2989, 2053, 2062, 2084, 2531, 1010, 2199, 1011, 14087, 3229, 2000, 2049, 2087, 8995, 3791, 1998, 5344, 1996, 4487, 11493, 2618, 29397, 1997, 2172, 1997, 2049, 3019, 4915, 11427, 2000, 1996, 3439, 3627, 1997, 11424, 16136, 20152, 1010, 1998, 2087, 3728, 2000, 1037, 2828, 1997, 23226, 20181, 2008, 2001, 16021, 6132, 13043, 2000, 2529, 13372, 1012, 102], [101, 2057, 2024, 2036, 8794, 2005, 1996, 6958, 2094, 2597, 2008, 2035, 5014, 2442, 2022, 7183, 1010, 2008, 3785, 2442, 2022, 2580, 2005, 1996, 2709, 1997, 2035, 1997, 2216, 2040, 2031, 2042, 10016, 1998, 2008, 1996, 16757, 1999, 11491, 1998, 2777, 11631, 14713, 2442, 2022, 12361, 1996, 2157, 2000, 2166, 1998, 2000, 4071, 1997, 2929, 1998, 2035, 2942, 1010, 2120, 1998, 3412, 22467, 1012, 102, 0, 0, 0, 0, 0, 0, 0], [101, 2009, 2097, 2022, 14203, 2005, 2033, 2000, 23120, 2588, 1996, 3809, 3171, 3471, 5307, 1996, 4975, 3032, 1998, 1996, 4945, 1997, 2035, 4740, 2000, 4867, 1996, 2412, 1011, 17973, 6578, 2090, 1996, 2764, 1998, 1996, 4975, 3032, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3577, 2097, 2507, 11119, 9584, 2000, 2151, 4073, 2029, 2089, 2022, 2081, 2247, 2122, 3210, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(X_train_short.to_list(), padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e0d51ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_short.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  mode='min',\n",
    "                                                  min_delta=0,\n",
    "                                                  patience=0,\n",
    "                                                  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6bb9c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 6/50 [==>...........................] - ETA: 16:03 - loss: 0.6751 - sparse_categorical_accuracy: 0.5365"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      2\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m),\n\u001b[1;32m      3\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy(),\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX_train_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_attention\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m#, batch_size=params['BATCH_SIZE']\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m#, steps_per_epoch=params['NUM_STEPS']\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_valid_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid_attention\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m#, callbacks=[early_stopping]\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m#, verbose=2\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m         \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/.local/lib/python3.8/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(x=[X_train_ids, X_train_attention]\n",
    "          , y=y_train.to_numpy()\n",
    "          , epochs=3\n",
    "          #, batch_size=params['BATCH_SIZE']\n",
    "          #, steps_per_epoch=params['NUM_STEPS']\n",
    "          , validation_data=([X_valid_ids, X_valid_attention], y_valid.to_numpy())\n",
    "          #, callbacks=[early_stopping]\n",
    "          #, verbose=2\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5346bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06e95b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f308453fc40>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ec47209",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaved_model/my_model2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/mnt/ceph/storage/data-tmp/teaching-current/aq60qovu/.local/lib/python3.8/site-packages/keras/saving/save.py:142\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     (h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile)) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     saving_utils\u001b[38;5;241m.\u001b[39mis_hdf5_filepath(filepath)):\n\u001b[1;32m    139\u001b[0m   \u001b[38;5;66;03m# TODO(b/130258301): add utility method for detecting model type.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m  \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[1;32m    141\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, sequential\u001b[38;5;241m.\u001b[39mSequential)):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubclassed models, because such models are defined via the body of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma Python method, which isn\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt safely serializable. Consider saving \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto the Tensorflow SavedModel format (by setting save_format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor using `save_weights`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    149\u001b[0m   hdf5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[1;32m    150\u001b[0m       model, filepath, overwrite, include_optimizer)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "model.save('saved_model/my_model2', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10080740",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf450ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
