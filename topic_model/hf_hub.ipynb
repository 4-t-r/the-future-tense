{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2ad8e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import random\n",
    "import scikitplot as skplt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7e8c3b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../datasets/future_statements_dataset/X_train.csv')[\"statement\"]\n",
    "y_train = pd.read_csv('../datasets/future_statements_dataset/y_train.csv')[\"future\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5d12b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'MAX_LENGTH': 128,\n",
    "          'EPOCHS': 50,\n",
    "          #learningrate\n",
    "          'LEARNING_RATE': 5e-5,\n",
    "          'FT_EPOCHS': 10,\n",
    "          'OPTIMIZER': 'adam',\n",
    "          'FL_GAMMA': 2.0,\n",
    "          'FL_ALPHA': 0.2,\n",
    "          'BATCH_SIZE': 64,\n",
    "          'NUM_STEPS': len(X_train.index) // 64,\n",
    "          #dropouts:\n",
    "          'DISTILBERT_DROPOUT': 0.2,\n",
    "          'DISTILBERT_ATT_DROPOUT': 0.2,\n",
    "          'LAYER_DROPOUT': 0.2,\n",
    "          'KERNEL_INITIALIZER': 'GlorotNormal',\n",
    "          'BIAS_INITIALIZER': 'zeros',\n",
    "          'POS_PROBA_THRESHOLD': 0.90,\n",
    "          'ADDED_LAYERS': 'Dense 256, Dense 32, Dropout 0.2',\n",
    "          'LR_SCHEDULE': '5e-5 for 6 epochs, Fine-tune w/ adam for 2 epochs @2e-5',\n",
    "          'FREEZING': 'All DistilBERT layers frozen for 6 epochs, then unfrozen for 2',\n",
    "          'CALLBACKS': '[early_stopping w/ patience=0]',\n",
    "          'RANDOM_STATE': 42\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "68b1e103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_projector', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_59', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9e579f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples[\"statement\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4ca1d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode(_tokenizer, texts, batch_size=256, max_length=params['MAX_LENGTH']):\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "    A function that encodes a batch of texts and returns the texts'\n",
    "    corresponding encodings and attention masks that are ready to be fed\n",
    "    into a pre-trained transformer model.\n",
    "    Input:\n",
    "        - _tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
    "        - texts:       List of strings where each string represents a text\n",
    "        - batch_size:  Integer controlling number of texts in a batch\n",
    "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
    "    Output:\n",
    "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
    "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
    "    \"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = _tokenizer.batch_encode_plus(batch,\n",
    "                                              max_length=max_length,\n",
    "                                              padding='max_length',\n",
    "                                              truncation=True,\n",
    "                                              return_attention_mask=True,\n",
    "                                              return_token_type_ids=False\n",
    "                                              )\n",
    "        input_ids.extend(inputs['input_ids'])\n",
    "        attention_mask.extend(inputs['attention_mask'])\n",
    "\n",
    "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "47717e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "17377fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "# Create train/validation split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aafbf35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:    1600  rows. Negatives: 810 Positives: 790\n",
      "Validation data:  400  rows. Negatives: 180 Positives: 220\n",
      "Test data:        500  rows. Negatives: 260 Positives: 240\n"
     ]
    }
   ],
   "source": [
    "# Sort index\n",
    "X_train.sort_index(inplace=True)\n",
    "X_valid.sort_index(inplace=True)\n",
    "X_test.sort_index(inplace=True)\n",
    "y_train.sort_index(inplace=True)\n",
    "y_valid.sort_index(inplace=True)\n",
    "y_test.sort_index(inplace=True)\n",
    "\n",
    "# Reset index\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_valid.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_valid.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('Training data:   ', len(X_train.index), ' rows. Negatives:', (y_train==0).sum(), 'Positives:', (y_train==1).sum())\n",
    "print('Validation data: ', len(X_valid.index), ' rows. Negatives:', (y_valid==0).sum(), 'Positives:', (y_valid==1).sum())\n",
    "print('Test data:       ', len(X_test.index), ' rows. Negatives:', (y_test==0).sum(), 'Positives:', (y_test==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "70e1a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode X_train\n",
    "X_train_ids, X_train_attention = batch_encode(tokenizer, X_train.tolist())\n",
    "\n",
    "# Encode X_valid\n",
    "X_valid_ids, X_valid_attention = batch_encode(tokenizer, X_valid.tolist())\n",
    "\n",
    "# Encode X_test\n",
    "X_test_ids, X_test_attention = batch_encode(tokenizer, X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8b0d6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_short = X_train[[1, 2, 501, 502]]\n",
    "y_train_short = y_train[[1, 2, 501, 502]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8cb81a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1996, 2783, 2095, 2038, 2042, 2053, 6453, 2000, 2008, 6517, 5418, 1998, 9272, 3613, 2000, 2022, 1996, 4539, 1997, 4491, 1024, 1999, 5712, 1010, 2119, 8956, 1998, 3097, 10342, 2031, 2042, 9416, 1025, 1999, 6921, 1010, 2019, 2886, 2001, 2566, 22327, 9250, 2114, 1037, 14334, 3345, 1025, 1998, 1010, 1999, 1996, 2845, 4657, 1010, 2045, 2031, 2042, 1037, 5164, 1997, 4491, 1010, 1996, 6745, 2108, 1996, 2082, 9288, 1999, 2022, 14540, 2319, 1010, 2073, 2336, 3062, 6778, 2000, 2019, 15741, 2012, 21735, 1012, 102], [101, 2005, 1996, 2034, 2051, 1999, 2256, 2381, 1010, 2256, 2455, 6645, 2031, 2042, 4225, 1010, 1998, 2027, 2024, 2085, 2108, 9530, 23633, 2135, 17183, 2906, 12921, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2256, 2784, 17005, 2005, 2017, 2004, 1037, 2270, 3275, 1998, 1037, 17689, 14306, 2015, 2149, 2008, 2023, 5219, 2104, 2115, 8606, 2097, 2693, 3741, 2013, 4867, 2120, 13120, 2015, 2875, 17700, 13120, 2015, 1997, 4071, 1998, 3425, 2005, 2035, 14938, 1998, 1997, 11074, 9945, 2005, 2035, 2163, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2010, 2376, 1998, 4621, 2435, 2140, 2097, 2146, 2022, 4622, 2011, 2035, 2040, 2938, 2182, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(X_train_short.to_list(), padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8e0d51ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_short.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ddf0661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  mode='min',\n",
    "                                                  min_delta=0,\n",
    "                                                  patience=0,\n",
    "                                                  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f6bb9c14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already trained weights available...\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "if not os.path.exists('./checkpoints/'):\n",
    "    print('Train model...')\n",
    "    model.fit(x=[X_train_ids, X_train_attention]\n",
    "              , y=y_train.to_numpy()\n",
    "              , epochs=3\n",
    "              #, batch_size=params['BATCH_SIZE']\n",
    "              #, steps_per_epoch=params['NUM_STEPS']\n",
    "              , validation_data=([X_valid_ids, X_valid_attention], y_valid.to_numpy())\n",
    "              #, callbacks=[early_stopping]\n",
    "              , verbose=1\n",
    "             )\n",
    "    model.save_weights('./checkpoints/my_checkpoint')\n",
    "else:\n",
    "    print('Already trained weights available...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8/16 [==============>...............] - ETA: 57s - loss: 0.6877 - sparse_categorical_accuracy: 0.5234 "
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x=[X_test_ids, X_test_attention], y=y_test, verbose=1)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8290e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not y_pred:\n",
    "    y_pred = model.predict([X_test_ids, X_test_attention]\n",
    "                           , verbose=1)\n",
    "prediction_logits = y_pred[0]\n",
    "prediction_probs = tf.nn.softmax(prediction_logits,axis=1).numpy()\n",
    "y_pred_thresh = np.where(prediction_probs > params['POS_PROBA_THRESHOLD'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t = pd.Series([el[1] for el in y_pred_thresh])\n",
    "\n",
    "# Get evaluation results\n",
    "accuracy = accuracy_score(y_test, y_pred_t)\n",
    "auc_roc = roc_auc_score(y_test, y_pred_t)\n",
    "\n",
    "print(accuracy)\n",
    "print(auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39babb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_pred_t.tolist()\n",
    "                                    , y_test.to_list()\n",
    "                                    , figsize=(6, 6)\n",
    "                                    , text_fontsize=14)\n",
    "plt.title(label='Test Confusion Matrix', fontsize=20, pad=17)\n",
    "plt.xlabel('Predicted Label', labelpad=14)\n",
    "plt.ylabel('True Label', labelpad=14)\n",
    "\n",
    "plt.savefig('../figures/future_statements_confusionmatrix.png', dpi=300.0, transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec47209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('saved_model/my_model2', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf450ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
