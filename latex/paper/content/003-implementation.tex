\section{Implementation}
As previously described, our approach consists of three steps.
Initially, we outline the raw data acquiring containing AI expressions.
Towards this objective, we utilize the WARC-DL pipeline \citep{Deckers2022} to extract the data from the given web archive.
For the preparation of this data for analysis, we developed the Model Pipeline which sequentially applies three models to the AI sentences:
\begin{itemize}
    \item Future statements extraction model
    \item Sentiment assignment model
    \item Topic assignment model
\end{itemize}
The Model Pipeline and all the incorporated models are covered in detail below. This way the final data set is generated with the attributes future statement, sentiment and topic.
Finally, the latter created data set serves as a base for a sentiment analysis about the future of AI divided in several subtopics, which we will also address.

\subsection{WARC Data Extraction}
The Webis Group\footnote{\url{https://webis.de}} had provided us with access to 37908 WARC files, with a total amount of \highlight{XXXXXXTB} of website data, and a high-performance computer cluster where we were able to schedule jobs.
Additionally, we decided to utilize the WARC-DL pipeline \citep{Deckers2022}, a Python software pipeline tightly coupled with the WARC endpoint, and using the FastWARC\footnote{\url{https://resiliparse.chatnoir.eu/en/stable/man/fastwarc.html}} library under the hood to iterate over WARC records.
It enabled us to extract text automatically from the WARC records using several customizable filters.
We made some slight modifications to the source code, in order to make it fit our needs better.
\\
Since we were going to analyze AI statements, we did not need all of the text provided by a website html, but grammatically correct statements of the English language.
It is not possible to extract all statements with an accuracy of 100\% using regex only, so we narrowed it down to passages which started with a capital letter and ended with a period or exclamation mark.
We built a regex pattern which extracted a list of all valid statements from a html source.
\\
Furthermore, we were only interested in statements about AI, so we compiled a list of AI keywords beforehand and applied a second regex on the extracted statements.
We compiled a blacklist which contained keywords for filtering out hostnames related to pornographic websites, and a whitelist to only accept top level domains which were most common, and where the websites most likely contained English text.
This was necessary, since initial runs of the WARC-DL pipeline extracted lots of unusable content (including content in languages other than English).
\\
We ran into some problems with the cluster and WARC-DL pipeline, so we could not extract all statements in one job run.
Jobs would suddenly stop extraction because of connection errors or out-of-memory errors, and halt with an Exception.
We separated hostnames in four groups depending on the initial hostname character: \texttt{a-h}, \texttt{i-p}, \texttt{q-x} and \texttt{yz0-9}.
This way we could pin down the problematic websites more precisely.
In the end we managed to work through all WARC files in group \texttt{i-p} and \texttt{yz0-9}, and most of the WARC files in groups \texttt{a-h} and \texttt{q-x}.
The final yield of the WARC data extraction stage was a total amount of 222246 AI statements.
In the next stage, the model pipeline, our objective was to further refine this initial data set.

\subsection{Model Pipeline}
The model pipeline follows the WARC data extraction step and is designed to prepare our final data set, which consists of the future statements and their associated sentiment and topic labels.
The processing within the model pipeline is performed on batches of 30 records each from the WARC-DL output.
\\
First the future model filters out future statements from the corresponding batch.
Subsequently, the chosen sentiment model assigns a sentiment to each future statement.
In this step some future terms can be sorted out.
This concerns the statements to which a sentiment is classified with a probability of less than 70\%.
The remaining future statements receive a topic.
Finally those are persisted in a csv file.
\\
In the following sections \ref{future-model} - \ref{topic-model} we will go into detail about each individual model.
In this context, we describe how the future model was trained and justify our decision for the sentiment and the topic model selection.
Furthermore we outline the choice of our topics and explain, why only those statements are kept which a sentiment with a probability above 70\% can be attributed.

\subsection{Future Model}
\label{future-model}
Since this paper focuses on analyzing statements about the future, a system for distinguishing between future statements and other expressions is required.
In this context, we decided to finetune the DistilBERT \citep{Sanh2019DistilBERTAD} base model that accomplishes this task.
Therefore, in this subsection, the collection of appropriate training data and the subsequent finetuning of the corresponding model is thematized.

\subsubsection{Training Data Set}
\label{training}
In order to provide a suitable data set to establish the future model, we adopted multiple approaches.
At this point, our goal was to compose the data in such a way that we would have a balanced data set with two classes.
The first class should contain future statements and the second all other types of terms.
While two of our group members manually annotated 500 observations each, the other two used an automated mechanism with subsequent verification of the collected data.
\\
One of the automatized approaches involves a web crawler developed on the basis of the python library Beautiful Soup \citep{Richardson2022}.
The text on a page is divided into sentences.
Subsequently every sentence is examined for occurrence of certain terms, as \emph{going to}, \emph{will}, \emph{won't} or \emph{'ll}.
\\
The second automated approach is the sentence extraction tool, which works in several aspects, similar to the web crawler.
At the beginning, it searches the given directory for text files.
If those exist the text is split into sentences and observed for specific expressions, as described above.
\\
To find the phrases that are not future statements, both the web crawler and the sentence extraction tool look only at the corresponding records that do not contain the previously considered expressions.
A careful manual review of all terms gathered by the automated systems was subsequently performed to remove the incorrect records.
\\
Finally, we constructed a data set with 1250 future statements and 1250 other phrases that did not contain future statements.

\subsubsection{Training}
As previously described we used the DistilBERT base model and finetuned it with the data set specified in \ref{training}.
We split the data set of 2500 records into a training and a test set, where the test set contains 20\% of the records.
From the training set we split further 20\% for validation data.
\\
After only two epochs the training ended with an accuracy over 96\% as displayed in (Table~\ref{future-model-train}).
\\
Subsequently we tested the model on our test set containing 500 records never seen by the model and achieved an accuracy of 93.8\%, as seen in the confusion matrix in (Table~\ref{cm}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{rrrrr}
        \toprule
        \textbf{Epoch} & \textbf{Train Loss} & \textbf{Train Accuracy} & \textbf{Val. Loss} & \textbf{Val. Accuracy}\\
        \midrule
        0 & 0.3816 & 0.8594 & 0.1547 & 0.9475 \\
        1 & 0.1142 & 0.9613 & 0.1272 & 0.9625 \\
        \bottomrule
    \end{tabular}
    }
\caption{\label{future-model-train}
Training Results
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[b]
    \centering
    \tiny
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lrr}
        \toprule
        {} & 1 & 2 \\
        \midrule
        True future statements & 232 & 13 \\
        True non-future statements & 18 & 237 \\
        \bottomrule
    \end{tabular}
    }
\caption{\label{cm}
Confusion Matrix\\
column 1: classified as future statements\\
column 2: classified as no future statements
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Sentiment Model}
In order to assign sentiments to future statements for later analysis, we decided to select a ready-trained model.
The chosen sentiment model is the SentimentAnalyzer of the open-source library pysentimiento \citep{perez2021pysentimiento}, which was further trained on about 40k tweets.
It uses the BERTweet \citep{bertweet} as a base model, pre-trained on english tweets.

\subsubsection{Evaluation}
To evaluate the SentimentAnalyzer, we annotated 500 future statements, which were previously used for training the future model, as negative, positive or neutral and received an accuracy of about 65\%.
\\
We then analyzed all misclassified statements.
We noticed that some of the statements could not be assigned impartially to one of the three categories.
An Example is \emph{``AI will reinvent how we think about education''}.
In the case of the sentence, we disagreed on whether we should value the sentence as neutral or positive and decided to use the neutral label.
Subsequently, this statement was given a positive rating by the model.
On closer examination of the statements that were labeled differently by us and by the model, we found over 90\% of the labels given by the model to be valid, if these annotations were assigned with probability over 70\%.
For this reason, we decided to keep only statements about the future if the sentiment model assigned an annotation with a confidence above 70\%.

\subsection{Topic Model}
\label{topic-model}
